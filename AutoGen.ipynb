{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+Rcu/7rZqNDYra8M7BnWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/chatGPT/blob/main/AutoGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**AutoGen sample**"
      ],
      "metadata": {
        "id": "A2wlUyR6TnhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# APIの設定\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# APIキーの準備\n",
        "# #ngrok_aceess_token = key[5]\n",
        "#openai_api_key = key[3]\n",
        "# deepl_auth_key = key[1]\n",
        "# serp_api_key = key[7]\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = key[3]\n",
        "os.environ[\"SERPAPI_API_KEY\"] = key[7]\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = key[9]\n",
        "os.environ[\"GOOGLE_API_KEY\"] = key[11]"
      ],
      "metadata": {
        "id": "un5Fdt6cjG2L",
        "outputId": "76210cf3-a05c-42da-8a68-d64329a0d443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyautogen"
      ],
      "metadata": {
        "id": "ZcoZ_52noLdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "\n",
        "config_list = [\n",
        "    {\n",
        "        'model': 'gpt-4o-2024-05-13',\n",
        "        'api_key': os.environ[\"OPENAI_API_KEY\"],\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "GNxuqkywoRTk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_config = {\"config_list\": config_list, \"cache_seed\": 42}\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"User_proxy\",\n",
        "    system_message=\"A human admin.\",\n",
        "    code_execution_config={\n",
        "        \"last_n_messages\": 2,\n",
        "        \"work_dir\": \"groupchat\",\n",
        "        \"use_docker\": False,\n",
        "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
        "    human_input_mode=\"TERMINATE\",\n",
        ")\n",
        "coder = autogen.AssistantAgent(\n",
        "    name=\"Coder\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "pm = autogen.AssistantAgent(\n",
        "    name=\"Product_manager\",\n",
        "    system_message=\"Creative in software product ideas.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
        "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
      ],
      "metadata": {
        "id": "lMOSHn2tp00V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy.initiate_chat(\n",
        "    manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
        ")\n",
        "# type exit to terminate the chat"
      ],
      "metadata": {
        "id": "zwt26okpqDrX",
        "outputId": "5d3f28af-347f-4e5a-90b4-dbad2aa1e938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User_proxy (to chat_manager):\n",
            "\n",
            "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Coder\n",
            "\n",
            "Coder (to chat_manager):\n",
            "\n",
            "Sure, I can help with that. I'll write a Python script to scrape arxiv.org for the latest paper related to GPT-4, and then we'll extract information regarding its potential applications in software.\n",
            "\n",
            "Here’s the plan:\n",
            "1. Scrape arxiv.org to find the latest paper related to GPT-4.\n",
            "2. Extract the title, authors, abstract, and link to the paper.\n",
            "3. From the abstract, identify potential applications in the field of software.\n",
            "\n",
            "Let's start with step 1 and 2, wherein we create a script to find the latest paper related to GPT-4.\n",
            "\n",
            "```python\n",
            "# filename: find_latest_gpt4_paper.py\n",
            "\n",
            "import requests\n",
            "from bs4 import BeautifulSoup\n",
            "\n",
            "def find_latest_gpt4_paper():\n",
            "    url = \"http://export.arxiv.org/api/query?search_query=all:gpt-4&start=0&max_results=1&sortBy=submittedDate&sortOrder=descending\"\n",
            "    response = requests.get(url)\n",
            "    soup = BeautifulSoup(response.content, 'xml')\n",
            "\n",
            "    entry = soup.find('entry')\n",
            "    if entry:\n",
            "        title = entry.find('title').text\n",
            "        authors = [author.find('name').text for author in entry.find_all('author')]\n",
            "        abstract = entry.find('summary').text\n",
            "        link = entry.find('id').text\n",
            "\n",
            "        print(\"Title:\", title)\n",
            "        print(\"Authors:\", \", \".join(authors))\n",
            "        print(\"Abstract:\", abstract)\n",
            "        print(\"Link:\", link)\n",
            "    else:\n",
            "        print(\"No papers found related to GPT-4.\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    find_latest_gpt4_paper()\n",
            "```\n",
            "\n",
            "Please save the script to a file named `find_latest_gpt4_paper.py` and execute it. This script will fetch and print details about the latest paper on GPT-4 from arxiv.org. Once you have the details, we can proceed to step 3 to identify potential applications in software based on the abstract.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: User_proxy\n",
            "\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "User_proxy (to chat_manager):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "Title: JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n",
            "  Against Large Language Models\n",
            "Authors: Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang\n",
            "Abstract:   Jailbreak attacks aim to induce Large Language Models (LLMs) to generate\n",
            "harmful responses for forbidden instructions, presenting severe misuse threats\n",
            "to LLMs. Up to now, research into jailbreak attacks and defenses is emerging,\n",
            "however, there is (surprisingly) no consensus on how to evaluate whether a\n",
            "jailbreak attempt is successful. In other words, the methods to assess the\n",
            "harmfulness of an LLM's response are varied, such as manual annotation or\n",
            "prompting GPT-4 in specific ways. Each approach has its own set of strengths\n",
            "and weaknesses, impacting their alignment with human values, as well as the\n",
            "time and financial cost. This diversity in evaluation presents challenges for\n",
            "researchers in choosing suitable evaluation methods and conducting fair\n",
            "comparisons across different jailbreak attacks and defenses. In this paper, we\n",
            "conduct a comprehensive analysis of jailbreak evaluation methodologies, drawing\n",
            "from nearly ninety jailbreak research released between May 2023 and April 2024.\n",
            "Our study introduces a systematic taxonomy of jailbreak evaluators, offering\n",
            "in-depth insights into their strengths and weaknesses, along with the current\n",
            "status of their adaptation. Moreover, to facilitate subsequent research, we\n",
            "propose JailbreakEval, a user-friendly toolkit focusing on the evaluation of\n",
            "jailbreak attempts. It includes various well-known evaluators out-of-the-box,\n",
            "so that users can obtain evaluation results with only a single command.\n",
            "JailbreakEval also allows users to customize their own evaluation workflow in a\n",
            "unified framework with the ease of development and comparison. In summary, we\n",
            "regard JailbreakEval to be a catalyst that simplifies the evaluation process in\n",
            "jailbreak research and fosters an inclusive standard for jailbreak evaluation\n",
            "within the community.\n",
            "\n",
            "Link: http://arxiv.org/abs/2406.09321v1\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Product_manager\n",
            "\n",
            "Product_manager (to chat_manager):\n",
            "\n",
            "Based on the abstract of the paper titled \"JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models,\" we can derive several potential applications in the software field. Here are some of them:\n",
            "\n",
            "### Potential Applications in Software:\n",
            "\n",
            "1. **Security Tools for LLMs:**\n",
            "   - **Jailbreak Detection Software:** Develop software solutions that integrate JailbreakEval to monitor and identify potential jailbreak attempts in real-time, enhancing the security of deployed language models.\n",
            "   - **Automated Defense Mechanisms:** Create automated systems that leverage JailbreakEval to detect harmful responses and take preemptive actions to mitigate potential misuse.\n",
            "\n",
            "2. **Evaluation Frameworks:**\n",
            "   - **Standardized Evaluation Platforms:** Build platforms that use JailbreakEval to provide standardized, consistent evaluations of language models. This could be targeted towards researchers and developers who need to assess model robustness and safety.\n",
            "   - **Benchmarking Tools:** Develop tools that allow for extensive benchmarking of different language models against various jailbreak attempts, facilitating cross-comparison and improvement.\n",
            "\n",
            "3. **Research Facilitation:**\n",
            "   - **Research Support Software:** Design software that assists researchers studying jailbreak techniques by automating the evaluation process using JailbreakEval, saving time and resources.\n",
            "   - **Collaborative Research Platforms:** Create online platforms where researchers can share their findings and evaluations using a standardized setup provided by JailbreakEval.\n",
            "\n",
            "4. **Custom Evaluation Systems:**\n",
            "   - **Configurable Evaluation Solutions:** Develop user-friendly software that lets users customize their evaluation workflows within a unified framework, catering to specific research needs or practical applications.\n",
            "   - **SaaS for Jailbreak Evaluation:** Offer a Software-as-a-Service (SaaS) model where users can easily evaluate language models against jailbreak attempts using JailbreakEval, without needing extensive setup or technical knowledge.\n",
            "\n",
            "5. **Education and Training:**\n",
            "   - **Educational Tools:** Create interactive educational tools that demonstrate different jailbreak attempts and defenses using JailbreakEval, aimed at training AI safety professionals and researchers.\n",
            "   - **Training Simulations:** Develop simulation environments that use JailbreakEval to provide hands-on experience with evaluating and defending against jailbreak attempts.\n",
            "\n",
            "6. **Compliance and Ethics:**\n",
            "   - **Compliance Monitoring Software:** Implement software that helps organizations ensure compliance with AI safety standards by routinely evaluating their deployed models against known jailbreak attacks.\n",
            "   - **Ethical Assessment Tools:** Develop tools that use JailbreakEval to ensure that language models align with ethical guidelines and do not produce harmful or unauthorized content.\n",
            "\n",
            "### Example Workflow:\n",
            "\n",
            "Here is an example of how a potential software application could integrate JailbreakEval:\n",
            "\n",
            "1. **Setup:**\n",
            "   - Install JailbreakEval as part of the software’s dependencies.\n",
            "2. **Evaluation:**\n",
            "   - Use JailbreakEval to conduct regular evaluations of a language model’s responses, focusing on detecting and addressing jailbreak attempts.\n",
            "3. **Reporting:**\n",
            "   - Generate detailed reports summarizing the evaluation results, highlighting any vulnerabilities discovered and recommending fixes.\n",
            "4. **Mitigation:**\n",
            "   - Automatically update model defenses based on the evaluation outcomes, ensuring a proactive approach to security.\n",
            "\n",
            "### Conclusion:\n",
            "\n",
            "The paper introduces JailbreakEval as a comprehensive toolkit for evaluating jailbreak attempts against large language models, offering significant potential for various software applications aimed at improving model security, standardizing evaluations, and supporting research. By embedding such a toolkit within different types of software solutions, we can enhance the robustness and safety of AI systems across the board.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Coder\n",
            "\n",
            "Coder (to chat_manager):\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'Find a latest paper about gpt-4 on arxiv and find its potential applications in software.', 'role': 'assistant'}, {'content': 'Sure, I can help with that. I\\'ll write a Python script to scrape arxiv.org for the latest paper related to GPT-4, and then we\\'ll extract information regarding its potential applications in software.\\n\\nHere’s the plan:\\n1. Scrape arxiv.org to find the latest paper related to GPT-4.\\n2. Extract the title, authors, abstract, and link to the paper.\\n3. From the abstract, identify potential applications in the field of software.\\n\\nLet\\'s start with step 1 and 2, wherein we create a script to find the latest paper related to GPT-4.\\n\\n```python\\n# filename: find_latest_gpt4_paper.py\\n\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\ndef find_latest_gpt4_paper():\\n    url = \"http://export.arxiv.org/api/query?search_query=all:gpt-4&start=0&max_results=1&sortBy=submittedDate&sortOrder=descending\"\\n    response = requests.get(url)\\n    soup = BeautifulSoup(response.content, \\'xml\\')\\n\\n    entry = soup.find(\\'entry\\')\\n    if entry:\\n        title = entry.find(\\'title\\').text\\n        authors = [author.find(\\'name\\').text for author in entry.find_all(\\'author\\')]\\n        abstract = entry.find(\\'summary\\').text\\n        link = entry.find(\\'id\\').text\\n\\n        print(\"Title:\", title)\\n        print(\"Authors:\", \", \".join(authors))\\n        print(\"Abstract:\", abstract)\\n        print(\"Link:\", link)\\n    else:\\n        print(\"No papers found related to GPT-4.\")\\n\\nif __name__ == \"__main__\":\\n    find_latest_gpt4_paper()\\n```\\n\\nPlease save the script to a file named `find_latest_gpt4_paper.py` and execute it. This script will fetch and print details about the latest paper on GPT-4 from arxiv.org. Once you have the details, we can proceed to step 3 to identify potential applications in software based on the abstract.', 'name': 'Coder', 'role': 'user'}, {'content': \"exitcode: 0 (execution succeeded)\\nCode output: \\nTitle: JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\\n  Against Large Language Models\\nAuthors: Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang\\nAbstract:   Jailbreak attacks aim to induce Large Language Models (LLMs) to generate\\nharmful responses for forbidden instructions, presenting severe misuse threats\\nto LLMs. Up to now, research into jailbreak attacks and defenses is emerging,\\nhowever, there is (surprisingly) no consensus on how to evaluate whether a\\njailbreak attempt is successful. In other words, the methods to assess the\\nharmfulness of an LLM's response are varied, such as manual annotation or\\nprompting GPT-4 in specific ways. Each approach has its own set of strengths\\nand weaknesses, impacting their alignment with human values, as well as the\\ntime and financial cost. This diversity in evaluation presents challenges for\\nresearchers in choosing suitable evaluation methods and conducting fair\\ncomparisons across different jailbreak attacks and defenses. In this paper, we\\nconduct a comprehensive analysis of jailbreak evaluation methodologies, drawing\\nfrom nearly ninety jailbreak research released between May 2023 and April 2024.\\nOur study introduces a systematic taxonomy of jailbreak evaluators, offering\\nin-depth insights into their strengths and weaknesses, along with the current\\nstatus of their adaptation. Moreover, to facilitate subsequent research, we\\npropose JailbreakEval, a user-friendly toolkit focusing on the evaluation of\\njailbreak attempts. It includes various well-known evaluators out-of-the-box,\\nso that users can obtain evaluation results with only a single command.\\nJailbreakEval also allows users to customize their own evaluation workflow in a\\nunified framework with the ease of development and comparison. In summary, we\\nregard JailbreakEval to be a catalyst that simplifies the evaluation process in\\njailbreak research and fosters an inclusive standard for jailbreak evaluation\\nwithin the community.\\n\\nLink: http://arxiv.org/abs/2406.09321v1\\n\", 'role': 'assistant'}, {'content': 'Based on the abstract of the paper titled \"JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models,\" we can derive several potential applications in the software field. Here are some of them:\\n\\n### Potential Applications in Software:\\n\\n1. **Security Tools for LLMs:**\\n   - **Jailbreak Detection Software:** Develop software solutions that integrate JailbreakEval to monitor and identify potential jailbreak attempts in real-time, enhancing the security of deployed language models.\\n   - **Automated Defense Mechanisms:** Create automated systems that leverage JailbreakEval to detect harmful responses and take preemptive actions to mitigate potential misuse.\\n\\n2. **Evaluation Frameworks:**\\n   - **Standardized Evaluation Platforms:** Build platforms that use JailbreakEval to provide standardized, consistent evaluations of language models. This could be targeted towards researchers and developers who need to assess model robustness and safety.\\n   - **Benchmarking Tools:** Develop tools that allow for extensive benchmarking of different language models against various jailbreak attempts, facilitating cross-comparison and improvement.\\n\\n3. **Research Facilitation:**\\n   - **Research Support Software:** Design software that assists researchers studying jailbreak techniques by automating the evaluation process using JailbreakEval, saving time and resources.\\n   - **Collaborative Research Platforms:** Create online platforms where researchers can share their findings and evaluations using a standardized setup provided by JailbreakEval.\\n\\n4. **Custom Evaluation Systems:**\\n   - **Configurable Evaluation Solutions:** Develop user-friendly software that lets users customize their evaluation workflows within a unified framework, catering to specific research needs or practical applications.\\n   - **SaaS for Jailbreak Evaluation:** Offer a Software-as-a-Service (SaaS) model where users can easily evaluate language models against jailbreak attempts using JailbreakEval, without needing extensive setup or technical knowledge.\\n\\n5. **Education and Training:**\\n   - **Educational Tools:** Create interactive educational tools that demonstrate different jailbreak attempts and defenses using JailbreakEval, aimed at training AI safety professionals and researchers.\\n   - **Training Simulations:** Develop simulation environments that use JailbreakEval to provide hands-on experience with evaluating and defending against jailbreak attempts.\\n\\n6. **Compliance and Ethics:**\\n   - **Compliance Monitoring Software:** Implement software that helps organizations ensure compliance with AI safety standards by routinely evaluating their deployed models against known jailbreak attacks.\\n   - **Ethical Assessment Tools:** Develop tools that use JailbreakEval to ensure that language models align with ethical guidelines and do not produce harmful or unauthorized content.\\n\\n### Example Workflow:\\n\\nHere is an example of how a potential software application could integrate JailbreakEval:\\n\\n1. **Setup:**\\n   - Install JailbreakEval as part of the software’s dependencies.\\n2. **Evaluation:**\\n   - Use JailbreakEval to conduct regular evaluations of a language model’s responses, focusing on detecting and addressing jailbreak attempts.\\n3. **Reporting:**\\n   - Generate detailed reports summarizing the evaluation results, highlighting any vulnerabilities discovered and recommending fixes.\\n4. **Mitigation:**\\n   - Automatically update model defenses based on the evaluation outcomes, ensuring a proactive approach to security.\\n\\n### Conclusion:\\n\\nThe paper introduces JailbreakEval as a comprehensive toolkit for evaluating jailbreak attempts against large language models, offering significant potential for various software applications aimed at improving model security, standardizing evaluations, and supporting research. By embedding such a toolkit within different types of software solutions, we can enhance the robustness and safety of AI systems across the board.', 'name': 'Product_manager', 'role': 'user'}, {'content': 'TERMINATE', 'name': 'Coder', 'role': 'user'}], summary='', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}