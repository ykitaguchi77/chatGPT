{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcY76pEaxC6B6yQLgTdtlP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/chatGPT/blob/main/ChatGPT_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Connect website information using LlamaIndex**\n",
        "\n",
        "参考：　https://note.com/npaka/n/n8c3867a55837\n",
        "\n",
        "https://github.com/wombyz/custom-knowledge-chatbot/blob/main/custom-knowledge-chatbot/Custom%20Knowledge%20Chatbot.ipynb\n",
        "\n",
        "https://qiita.com/sakasegawa/items/d01dafdf0c77da133f24\n",
        "\n",
        "https://qiita.com/sakasegawa/items/16714fa132e874cab069\n",
        "\n"
      ],
      "metadata": {
        "id": "D_VwfFD2bedd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dba_q2nDbB1g",
        "outputId": "deb809ee-fcfa-4dda-8066-2466ea1f2d89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/140.7 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.0/396.0 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama_index (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# パッケージのインストール\n",
        "!pip install llama_index --q\n",
        "!pip install langchain --q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# APIの設定\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# APIキーの準備\n",
        "# #ngrok_aceess_token = key[5]\n",
        "#openai_api_key = key[3]\n",
        "# deepl_auth_key = key[1]\n",
        "# serp_api_key = key[7]\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = key[3]\n",
        "os.environ[\"SERPAPI_API_KEY\"] = key[7]\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = key[9]\n",
        "os.environ[\"GOOGLE_API_KEY\"] = key[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5lb4bLTb6Ju",
        "outputId": "47a4ba19-9223-4252-cae7-1417c3b95be9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LlmaIndex**"
      ],
      "metadata": {
        "id": "S9Tnbei1RKIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ログレベルの設定\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\"\"\"\n",
        "・DEBUG : デバッグ情報の出力。\n",
        "・INFO : 想定通りのことが発生。\n",
        "・WARNING : 想定外のことが発生。\n",
        "・ERROR : 機能を実行できない。\n",
        "\"\"\"\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO, force=True)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "KeVcDfHCdDaq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data folderの作成\n",
        "# 教育データをtxtで載せる\n",
        "import shutil\n",
        "\n",
        "data_dir = \"/content/data\" \n",
        "if os.path.exists(data_dir):\n",
        "    shutil.rmtree(data_dir)\n",
        "os.makedirs(data_dir)\n",
        "shutil.copy(\"/content/drive/MyDrive/Deep_learning/chatGPT/jyujyutu.txt\", data_dir) #呪術廻戦のあらすじをdata folderに入れる"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o4Tm5nkteFM3",
        "outputId": "1fd43c26-7c9e-47de-b135-05db1fbcf470"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data/jyujyutu.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# インデックスの作成\n",
        "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "index = GPTSimpleVectorIndex(documents)"
      ],
      "metadata": {
        "id": "kWDVcG7UfE9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9056e132-bc8c-467d-cc14-92899dca59b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "NumExpr defaulting to 2 threads.\n",
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 101755 tokens\n",
            "> [build_index_from_documents] Total embedding token usage: 101755 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 質問応答\n",
        "print(index.query(\"乙骨が呪術師を目指すようになった理由\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7SvjWgffYiY",
        "outputId": "0593ce8e-3f10-4783-c395-338f55a1b498"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4737 tokens\n",
            "> [query] Total LLM token usage: 4737 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 30 tokens\n",
            "> [query] Total embedding token usage: 30 tokens\n",
            "\n",
            "\n",
            "乙骨が呪術師を目指すようになった理由は、真希が周囲を不本意に傷つけてしまっていたことを気にかけ、彼女が呪言を使う能力を持っていたため、意図せず人を呪うことを防ぐために会話の語彙をおにぎりの具に限定していたことを考慮したからです。また、渋谷事変では他の京都校の生徒と共に渋谷へと向かうなど、呪術師としての責任を果たすためにも、呪術師を\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 質問応答\n",
        "print(index.query(\"虎杖悠仁と一番仲がいいのは？\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c78IsKDnfzQk",
        "outputId": "9a4cdd27-9c6e-4d6d-c200-07bbc4f650d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3861 tokens\n",
            "> [query] Total LLM token usage: 3861 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 24 tokens\n",
            "> [query] Total embedding token usage: 24 tokens\n",
            "\n",
            "恵（ふしぐろ めぐみ）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 質問応答\n",
        "print(index.query(\"五条悟にはなぜ攻撃が当たらないのか\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfovzswgg2lg",
        "outputId": "74a7e8a1-9a17-4e99-b789-4944f612b2ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3810 tokens\n",
            "> [query] Total LLM token usage: 3810 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 26 tokens\n",
            "> [query] Total embedding token usage: 26 tokens\n",
            "\n",
            "五条悟は、自身の攻撃に周りの人間を巻き込まないように気を配っており、必要な時には被害を最小限に抑えようと心がけているため、攻撃が当たらないのです。また、五条家の相伝の無下限呪術を使用することで、自身の周囲に術式によって現実化させた無限を作ることで攻撃を防いだり、応用して、瞬間移動や空中浮遊なども可能にしているため、攻\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexの保存と読み込み"
      ],
      "metadata": {
        "id": "LaT4MZBvhNSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# インデックスの保存\n",
        "index.save_to_disk(\"index.json\")"
      ],
      "metadata": {
        "id": "UdF-MMNXhMHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# インデックスの読み込み\n",
        "index = GPTSimpleVectorIndex.load_from_disk(\"index.json\")"
      ],
      "metadata": {
        "id": "PMrkmIOBhTTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama Hubを利用して様々なファイルをindexする"
      ],
      "metadata": {
        "id": "m5fPIy2RlLSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube_transcript_reader\n",
        "\n",
        "from llama_index import download_loader\n",
        "\n",
        "PubmedReader = download_loader(\"PubmedReader\")\n",
        "\n",
        "loader = PubmedReader()\n",
        "documents = loader.load_data(search_query='blepharochalasis')"
      ],
      "metadata": {
        "id": "SHd5Ec8zlSGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# インデックスの作成\n",
        "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
        "index = GPTSimpleVectorIndex(documents)"
      ],
      "metadata": {
        "id": "uMSklnQXnONc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 質問応答\n",
        "print(index.query(\"What is blepharochalasis?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M01q-R-5nPP8",
        "outputId": "b2cdaff8-83b6-456a-9bfe-55ba9de44633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Blepharochalasis is a condition characterized by lax eyelid skin, easily evertible upper eyelids, and reactive conjunctival changes, such as papillary conjunctivitis and photophobia. It is associated with FES and is often seen in middle-aged obese patients.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 質問応答\n",
        "print(index.query(\"Please explain an acute and late treatment for blepharochalasis in detail. Please add references with AMA style.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pugnNpSBnoNn",
        "outputId": "5efb04cb-9b4b-4625-aed7-84540cb64aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Blepharochalasis is a condition characterized by the laxity of the eyelid skin and the weakening of the underlying structures, resulting in a drooping of the eyelid. Treatment for blepharochalasis can be divided into acute and late treatments. \n",
            "\n",
            "Acute treatment for blepharochalasis typically involves the use of topical medications, such as corticosteroids, to reduce inflammation and swelling. Additionally, the use of topical antibiotics may be recommended to reduce the risk of infection. In some cases, surgery may be recommended to remove excess skin and fat from the eyelids.\n",
            "\n",
            "Late treatment for blepharochalasis typically involves the use of surgical techniques to tighten the eyelid skin and underlying structures. This may include a brow lift, blepharoplasty, or a combination of both. In some cases, the use of injectable fillers may be recommended to restore volume to the area.\n",
            "\n",
            "References:\n",
            "\n",
            "Kumar, A., & Chaudhary, S. (2020). Blepharochalasis: A Review. Indian Journal of Ophthalmology, 68(7), 1445-1450. doi:10.4103/ijo.ijo_14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oOZEm2OSnuVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ChatGPT apiを用いる**"
      ],
      "metadata": {
        "id": "l26WL1l5L0ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Setup your LLM\n",
        "from llama_index import LLMPredictor, GPTSimpleVectorIndex, PromptHelper\n",
        "\n",
        "\n",
        "# define LLM\n",
        "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo\"))\n",
        "\n",
        "# define prompt helper\n",
        "# set maximum input size\n",
        "max_input_size = 4096\n",
        "# set number of output tokens\n",
        "num_output = 256\n",
        "# set maximum chunk overlap\n",
        "max_chunk_overlap = 20\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "\n",
        "custom_LLM_index = GPTSimpleVectorIndex(\n",
        "    documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
        ")"
      ],
      "metadata": {
        "id": "0YpLB5fVcAvd",
        "outputId": "4b58a1e5-34ae-4b54-936a-9be54f8e3b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 100100 tokens\n",
            "> [build_index_from_documents] Total embedding token usage: 100100 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "h2s6Pe7MfKK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query your index!\n",
        "\n",
        "response = custom_LLM_index.query(\"伏黒恵の式神についてまとめて下さい\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "wXAdWidtc65V",
        "outputId": "5f3f9d8c-8286-4d62-d3dc-df4b532ee0d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4011 tokens\n",
            "> [query] Total LLM token usage: 4011 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 24 tokens\n",
            "> [query] Total embedding token usage: 24 tokens\n",
            "\n",
            "\n",
            "The context information still does not provide any information about Fushiguro's shikigami.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "## ここまでは同じ  ##\n",
        "################\n",
        "\n",
        "# ログレベルの設定\n",
        "import logging\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "\"\"\"\n",
        "・DEBUG : デバッグ情報の出力。\n",
        "・INFO : 想定通りのことが発生。\n",
        "・WARNING : 想定外のことが発生。\n",
        "・ERROR : 機能を実行できない。\n",
        "\"\"\"\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO, force=True)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "\n",
        "# data folderの作成\n",
        "# 教育データをtxtで載せる\n",
        "data_dir = \"/content/data\" \n",
        "if os.path.exists(data_dir):\n",
        "    shutil.rmtree(data_dir)\n",
        "os.makedirs(data_dir)\n",
        "shutil.copy(\"/content/drive/MyDrive/Deep_learning/chatGPT/jyujyutu.txt\", data_dir) #呪術廻戦のあらすじをdata folderに入れる\n",
        "\n",
        "# インデックスの作成\n",
        "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "index = GPTSimpleVectorIndex(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLkJdq0NL4mX",
        "outputId": "ad01d5bd-8bfe-4540-ed84-9bed2e60a324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "INFO:root:> [build_index_from_documents] Total embedding token usage: 101755 tokens\n",
            "> [build_index_from_documents] Total embedding token usage: 101755 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
        "from llama_index.langchain_helpers.chatgpt import ChatGPTLLMPredictor\n",
        "\n",
        "# ChatGPTLLMPredictorの準備\n",
        "llm_predictor = ChatGPTLLMPredictor(\n",
        "    prepend_messages = [\n",
        "        {\"role\": \"system\", \"content\": \"あなたは役に立つアシスタントです。\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "# インデックスの作成\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "index = GPTSimpleVectorIndex(\n",
        "    documents=documents,\n",
        "    llm_predictor=llm_predictor\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svv7FoShMfE0",
        "outputId": "d5b36d65-9144-485f-dacd-9cf92b9c6026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "INFO:root:> [build_index_from_documents] Total embedding token usage: 102523 tokens\n",
            "> [build_index_from_documents] Total embedding token usage: 102523 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_axHmWPtUPrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kn-MdOraWioR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65ohLzgOWiqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chat-GPT API**\n",
        "https://fuji-pocketbook.net/chatgpt-api-python/"
      ],
      "metadata": {
        "id": "_N0TodShW3D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = key[3]\n",
        "\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"語頭には「ふむ。」、すべての語尾に「じゃ」か「のじゃ。」をつけて質問に短く答えてください\"},\n",
        "        {\"role\": \"user\", \"content\": \"APIってなに？\"},\n",
        "    ]   \n",
        ")\n",
        "print(f\"ChatGPT: {response['choices'][0]['message']['content']}\")\n",
        "print(response['usage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZvmmdK3WisI",
        "outputId": "781628c8-be6a-40e1-e31a-df296f760f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT: Webサービスの機能を提供するためのプログラムのことじゃ。\n",
            "{\n",
            "  \"completion_tokens\": 29,\n",
            "  \"prompt_tokens\": 71,\n",
            "  \"total_tokens\": 100\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"語頭には「ふむ。」、すべての語尾に「じゃ。」か「のじゃ」をつけて質問に短く答えてください\"},\n",
        "        {\"role\": \"user\", \"content\": \"APIってなに？\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"情報をやり取りするためのインターフェースのことじゃ。\"},\n",
        "        {\"role\": \"user\", \"content\": \"具体的にどこで使われているの？\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"ChatGPT: {response['choices'][0]['message']['content']}\")\n",
        "print(response['usage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aReWSamwWiuL",
        "outputId": "e3e37159-11f4-4453-c3bf-f66543a47579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT: Webアプリケーションやモバイルアプリなどのプログラム間で情報のやり取りをする場合によく使われているじゃよ。また、クラウドサービスやIoTなど様々な分野でも使われているのじゃ。\n",
            "{\n",
            "  \"completion_tokens\": 91,\n",
            "  \"prompt_tokens\": 120,\n",
            "  \"total_tokens\": 211\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ChatGPT API × LangChain**"
      ],
      "metadata": {
        "id": "Kcknaf1cVBgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://note.com/npaka/n/n403fc29a02c7\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# チャットモデルの準備\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "#　1メッセージによるチャットモデルの呼び出し\n",
        "chat([HumanMessage(content=\"「私はプログラミングが大好きです。」を日本語から英語に翻訳してください。\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvXnKv4PMo8v",
        "outputId": "cc786145-fd62-40c0-de78-1be530a88194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"I love programming.\"', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#　複数メッセージによるチャットモデルの呼び出し\n",
        "messages = [\n",
        "    SystemMessage(content=\"あなたは日本語を英語に翻訳するアシスタントです。\"),\n",
        "    HumanMessage(content=\"「私はプログラミングが大好きです。」を日本語から英語に翻訳してください。\")\n",
        "]\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxYpQEiaUYtI",
        "outputId": "9cd53b0e-430c-4843-a6ff-59efa7f94e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"I love programming.\"', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 複数メッセージセットによるチャットモデルの呼び出し\n",
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"あなたは日本語を英語に翻訳するアシスタントです。\"),\n",
        "        HumanMessage(content=\"「私はプログラミングが大好きです。」を日本語から英語に翻訳してください。\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"あなたは日本語を英語に翻訳するアシスタントです。\"),\n",
        "        HumanMessage(content=\"「私は人工知能が大好きです。」を日本語から英語に翻訳してください。\")\n",
        "    ],\n",
        "]\n",
        "chat.generate(batch_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrmBjRUfUgqV",
        "outputId": "125a9024-08c9-44e0-b0f6-53e2b944478f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text='\"I love programming.\"', generation_info=None, message=AIMessage(content='\"I love programming.\"', additional_kwargs={}))], [ChatGeneration(text='\"I love artificial intelligence.\"', generation_info=None, message=AIMessage(content='\"I love artificial intelligence.\"', additional_kwargs={}))]], llm_output=None)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **プロンプトテンプレート**"
      ],
      "metadata": {
        "id": "9UD2_hNGVHai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# systemメッセージプロンプトテンプレートの準備\n",
        "template=\"あなたは {input_language} を {output_language} に翻訳するアシスタントです。\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "# humanメッセージプロンプトテンプレートの準備\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "# chatプロンプトテンプレートの準備\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
      ],
      "metadata": {
        "id": "vzUQBNd9UtTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# プロンプトテンプレートによるチャットモデルの呼び出し\n",
        "chat(chat_prompt.format_prompt(\n",
        "    input_language=\"日本語\", \n",
        "    output_language=\"英語\", \n",
        "    text=\"私はプログラミングが大好きです。\"\n",
        ").to_messages())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7bGr59WU96g",
        "outputId": "075e0490-8bfb-4e95-f5a1-4c2d3efb8c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I love programming.', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# PromptTemplateからのsystemメッセージプロンプトテンプレートの作成\n",
        "prompt=PromptTemplate(\n",
        "    template=\"あなたは {input_language} を {output_language} に翻訳するアシスタントです。\",\n",
        "    input_variables=[\"input_language\", \"output_language\"],\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)"
      ],
      "metadata": {
        "id": "zu9KQwDZVRA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yn1KJQgrWTQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"ChatGPTへの指示\"},\n",
        "        {\"role\": \"user\", \"content\": \"会話の内容\"}\n",
        "    ]   \n",
        ")\n",
        "print(type(response))\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "k_rZamqQWTRu",
        "outputId": "ed3fe83c-4d60-4f3a-da92-3d65f233b37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7477bf662ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     messages=[\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ChatGPTへの指示\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"会話の内容\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'openai' is not defined"
          ]
        }
      ]
    }
  ]
}